{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nimport time\nimport warnings\nfrom itertools import combinations\nfrom warnings import simplefilter\n\nimport joblib\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\n\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\nis_offline = False\nis_train = True\nis_infer = True\nmax_lookback = np.nan\nsplit_day = 435\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:33.844214Z","iopub.execute_input":"2023-12-03T17:07:33.844497Z","iopub.status.idle":"2023-12-03T17:07:38.388933Z","shell.execute_reply.started":"2023-12-03T17:07:33.844472Z","shell.execute_reply":"2023-12-03T17:07:38.387974Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = df.dropna(subset=[\"target\"])\ndf.reset_index(drop=True, inplace=True)\ndf_shape = df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:38.391042Z","iopub.execute_input":"2023-12-03T17:07:38.391688Z","iopub.status.idle":"2023-12-03T17:07:56.813807Z","shell.execute_reply.started":"2023-12-03T17:07:38.391652Z","shell.execute_reply":"2023-12-03T17:07:56.812832Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:56.815034Z","iopub.execute_input":"2023-12-03T17:07:56.815338Z","iopub.status.idle":"2023-12-03T17:07:56.926384Z","shell.execute_reply.started":"2023-12-03T17:07:56.815296Z","shell.execute_reply":"2023-12-03T17:07:56.925083Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:56.927754Z","iopub.execute_input":"2023-12-03T17:07:56.928126Z","iopub.status.idle":"2023-12-03T17:07:57.568224Z","shell.execute_reply.started":"2023-12-03T17:07:56.928093Z","shell.execute_reply":"2023-12-03T17:07:57.567380Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def imbalance_features(df):\n    import cudf\n    df = cudf.from_pandas(df)\n    \n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n        \n    # V2 features\n    # Calculate additional features\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    # Calculate various statistical aggregation features\n    \n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    df = df.to_pandas()\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\ndef numba_imb_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    return df\n\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\ndef generate_all_features(df):\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    df = imbalance_features(df)\n    df = numba_imb_features(df)\n    df = other_features(df)\n    gc.collect() \n    \n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:57.570916Z","iopub.execute_input":"2023-12-03T17:07:57.571260Z","iopub.status.idle":"2023-12-03T17:07:57.589943Z","shell.execute_reply.started":"2023-12-03T17:07:57.571229Z","shell.execute_reply":"2023-12-03T17:07:57.588995Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    \n    # Display a message indicating offline mode and the shapes of the training and validation sets\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    \n    # Display a message indicating online mode\n    print(\"Online mode\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:57.591225Z","iopub.execute_input":"2023-12-03T17:07:57.591560Z","iopub.status.idle":"2023-12-03T17:07:57.603196Z","shell.execute_reply.started":"2023-12-03T17:07:57.591529Z","shell.execute_reply":"2023-12-03T17:07:57.602172Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Online mode\n","output_type":"stream"}]},{"cell_type":"code","source":"if is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:07:57.604581Z","iopub.execute_input":"2023-12-03T17:07:57.605048Z","iopub.status.idle":"2023-12-03T17:08:49.944667Z","shell.execute_reply.started":"2023-12-03T17:07:57.605015Z","shell.execute_reply":"2023-12-03T17:08:49.943826Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Build Online Train Feats Finished.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}